# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a PyBullet-based robotics simulation featuring a **Franka Panda robot arm** controlled by **Claude AI** through natural language commands. The system bridges natural language instructions with low-level robot control using vision analysis and spatial reasoning.

## Essential Commands

### Running the Application

```bash
# Batch mode - autonomous task execution with validation
python main.py --scene default
python main.py --scene scene_01

# Console interactive mode - terminal-based conversational control
python main_console.py --scene default

# Streamlit interactive mode - web-based UI (access at http://localhost:8501)
streamlit run main_interactive.py
```

### Development Setup

```bash
# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up environment variables (create .env file)
echo "ANTHROPIC_API_KEY=your_key_here" > .env
echo "ANTHROPIC_MODEL=claude-sonnet-4-5-20250929" >> .env
```

### Testing and Debugging

```bash
# List available scenes
ls scenes/*.yaml

# Monitor logs in real-time
tail -f logs/app_*.log

# Monitor API calls
tail -f logs/api_calls_*.log

# View generated panoramas
ls -lh images/
```

## Architecture

### Two Distinct Control Modes

The system has fundamentally different architectures for batch vs interactive modes:

#### 1. Batch Mode (`main.py`)
- **Purpose**: Autonomous execution with validated planning
- **Workflow**: Single LLM call → Local Python validation → Execution → Optional verification
- **Key Files**: `src/llm_validator.py`, `src/llm_controller.py`
- **Validation**: Python-based checks for plan structure, logical sequence, and constraints
- **Verification**: Optional post-execution verification using panorama analysis (controlled by `ENABLE_VERIFICATION` in config)

#### 2. Interactive Modes (`main_console.py` and `main_interactive.py`)
- **Purpose**: Conversational control with tool calling
- **Workflow**: User message → Claude with tools → Tool execution → Response
- **Key File**: `src/interactive_llm_controller.py`
- **Tools**: 10 tools for querying scene state and controlling robot (see below)
- **Validation**: Human-in-the-loop

### Core Component Hierarchy

```
Scene YAML → SceneLoader → ObjectManager → PyBullet Simulation
                                                   ↓
                                          RobotController (IK, gripper, actions)
                                                   ↓
                                          CameraManager (5-view panoramas)
                                                   ↓
                              ┌──────────────────────────────────────┐
                              ↓                                      ↓
                      LLMValidator (batch)              InteractiveLLMController
                              ↓                                      ↓
                      LLMController (execution)                Tool execution
```

### Module Responsibilities

- **`src/config.py`**: Centralized configuration for all parameters (robot, physics, LLM, camera)
- **`src/robot_controller.py`**: Low-level robot control (IK solver, gripper, pick/place actions)
- **`src/llm_validator.py`**: Batch mode planning and validation (simplified single-call workflow)
- **`src/llm_controller.py`**: Executes validated plans by mapping commands to robot actions
- **`src/interactive_llm_controller.py`**: Tool-based conversational control for interactive modes
- **`src/object_manager.py`**: Object lifecycle (loading, position queries, dimensions)
- **`src/camera_manager.py`**: Multi-view panorama capture (5 views: 0°, 90°, 180°, 270°, top)
- **`src/scene_loader.py`**: YAML scene parsing and validation
- **`src/logger.py`**: Dual logging system (console via Rich + rotating file logs)

## Available Robot Commands

### Batch Mode Commands

Commands are generated by the LLM and executed sequentially:

- `pick_up(object_name)` - Pick up specified object
- `place(x, y, z)` - Place held object at coordinates (ground placement requires z=0)
- `place_on(target_object)` - Place held object on top of another object
- `rotate_gripper_90()` - Rotate gripper 90 degrees
- `reset_gripper_orientation()` - Reset gripper to default orientation

**Validation Rules**:
- Cannot pick_up while already holding an object
- Cannot place/place_on without picking up first
- Ground placements must have z=0
- All commands require proper JSON structure with action field

### Interactive Mode Tools

10 tools available to Claude during conversational control:

**Query Tools**:
- `get_gripper_position()` - Current end-effector position
- `get_all_objects()` - List all objects with positions and dimensions
- `get_object_position(object_name)` - Specific object position
- `get_panorama()` - Capture and return scene panorama (base64)

**Action Tools**:
- `pick_up(object_name)` - Pick up object
- `place(x, y, z)` - Place at coordinates
- `place_on(target_object)` - Place on another object
- `open_gripper()` - Open gripper
- `close_gripper()` - Close gripper
- `move_gripper(x, y, z)` - Move gripper to position

## Scene System

### Scene File Structure

Scenes are YAML files in `scenes/` directory:

```yaml
metadata:
  name: "Scene Name"
  description: "Scene description"

objects:
  - name: "red_cube"
    type: "cube"
    position: [0.3, 0.3, 0.05]  # [x, y, z] in meters
    color: [1, 0, 0, 1]         # RGBA (0-1 range)
    scale: 1.0

task:
  description: "Task to accomplish"
```

### Robot Workspace Limits

- **X-axis**: -0.5 to 0.7 meters
- **Y-axis**: -0.7 to 0.7 meters
- **Z-axis**: 0 to 0.8 meters

Objects outside this workspace cannot be manipulated.

### Creating New Scenes

1. Copy `scenes/default.yaml` to `scenes/new_scene.yaml`
2. Modify object positions (within workspace limits), colors, and task
3. Run with `python main.py --scene new_scene`

## Configuration System

All parameters centralized in `src/config.py`:

### Key Parameters to Adjust

**Robot Control**:
- `LINEAR_MOVEMENT_SPEED = 0.1` - Movement speed (m/s)
- `THRESHOLD_PRECISE = 0.001` - Position accuracy threshold (meters)
- `STABILIZATION_LOOP_STEPS = 2500` - Physics stabilization cycles

**LLM Settings**:
- `SIMPLIFIED_MODE = True` - Uses single-call workflow (always True)
- `INTERACTIVE_MAX_TOKENS = 4096` - Max tokens per interactive response
- `ENABLE_PROMPT_CACHING = True` - Use Anthropic prompt caching
- `ENABLE_VERIFICATION = True` - Enable post-execution task verification

**Camera**:
- `CAMERA_IMAGE_WIDTH = 640` - Resolution
- `PANORAMA_QUALITY = 75` - JPEG quality (affects API token usage)
- `CAMERA_YAW_ANGLES = [0, 90, 180, 270, 0]` - Panorama view angles

## Prompt Caching Strategy

The system uses Anthropic's prompt caching to reduce API costs by up to 90%:

### Batch Mode Caching

- **System prompts**: Cached on first call, reused across scenes
- **Panorama images**: Cached in verification stage if enabled
- Saves ~1500-2000 tokens per verification call

### Interactive Mode Caching

- **System prompt + tool definitions**: Cached on first message, reused for entire conversation
- Saves ~1500 tokens per message after the first
- Example: 10-message conversation saves ~78% on system prompt costs

### Configuration

Toggle caching in `src/config.py`:
```python
ENABLE_PROMPT_CACHING = True  # Enable/disable
PANORAMA_QUALITY = 75  # Lower = fewer tokens but worse quality
```

## Logging System

### Log Files

- `logs/app_YYYYMMDD_HHMMSS.log` - Application events, robot actions, errors
- `logs/api_calls_YYYYMMDD_HHMMSS.log` - Detailed LLM request/response traces with token usage

### Log Configuration

Control logging in `src/config.py`:
```python
LOG_API_CALLS = True  # Enable detailed API logging
LOGS_FOLDER = 'logs'
```

### Token Usage Tracking

Logs show cache performance:
```
[LLM] Request completed
  Input tokens: 250
  Cache creation tokens: 1842  # New content cached
  Cache read tokens: 0
  Output tokens: 156
```

Next request:
```
[LLM] Request completed
  Input tokens: 300
  Cache creation tokens: 0
  Cache read tokens: 1842  # Reused cached content
  Output tokens: 189
```

## Development Guidelines

### Adding New Robot Actions

1. **Add method to RobotController** (`src/robot_controller.py`)
2. **For batch mode**: Update `LLMController.execute_plan()` to handle new action
3. **For batch mode**: Update `prompts/simplified_system_prompt.txt` to document command
4. **For interactive mode**: Add tool definition in `InteractiveLLMController.get_tool_definitions()`
5. **For interactive mode**: Add execution handler in `InteractiveLLMController._execute_tool()`

### Modifying Prompts

Prompt files are in `prompts/` directory:
- `simplified_system_prompt.txt` - Batch mode planning with examples
- `simplified_user_prompt.txt` - Batch mode user prompt template
- `interactive_system_prompt.txt` - Interactive mode system prompt
- `verification_system_prompt.txt` - Post-execution verification system prompt
- `verification_user_prompt.txt` - Post-execution verification user prompt template

### Adding Object Types

Currently only cubes are supported. To add new shapes:

1. **Add geometry loader** in `ObjectManager` (`src/object_manager.py`)
2. **Update scene loader** to handle new type (`src/scene_loader.py`)
3. **Update prompts** to describe handling considerations

### Debugging Workflow

1. **Check logs**: `tail -f logs/app_*.log`
2. **View panoramas**: Generated images saved to `images/` directory
3. **Monitor API usage**: Check `logs/api_calls_*.log` for token counts
4. **Enable verbose logging**: Set `LOG_API_CALLS = True` in config
5. **Check validation**: Logs show each validation stage and errors

## Common Issues and Solutions

### Plan Validation Failures

If the LLM-generated plan fails local validation:
1. Check log output for specific validation errors
2. Common issues:
   - Attempting to place without picking first
   - Ground placement with z != 0
   - Invalid action names
   - Missing required fields in commands
3. The validation logic is in `LLMValidator._validate_plan_locally()`

### Robot Cannot Reach Object

- Verify object position is within workspace limits (see above)
- Check scene YAML has correct coordinates
- Ensure physics has stabilized (wait for `STABILIZATION_LOOP_STEPS`)

### Interactive Mode Not Responding

- Console mode runs physics continuously - don't block with long operations
- Streamlit caches simulation initialization - restart if needed
- Check PyBullet GUI isn't already running: `pkill -f pybullet`

### High API Costs

1. Enable prompt caching: `ENABLE_PROMPT_CACHING = True`
2. Reduce panorama quality: `PANORAMA_QUALITY = 50`
3. Lower max tokens: `INTERACTIVE_MAX_TOKENS = 2048`
4. Monitor token usage in logs

## Important Notes

- **Simplified Workflow**: The codebase uses a simplified single-call workflow with local Python validation. Any references to a "5-stage validation workflow" in external documentation are outdated.
- **Vision Analysis**: Batch mode does NOT use vision analysis - plans are generated from object position data only. Panoramas are only captured for post-execution verification.
- **Prompt Caching**: System prompts and images are marked with `cache_control: {"type": "ephemeral"}` for caching
- **Object Types**: Only cubes are currently supported; URDF/mesh loading for other shapes would require extension
- **Physics Stabilization**: Always run `STABILIZATION_LOOP_STEPS` before capturing panoramas or executing plans to ensure stable physics state
